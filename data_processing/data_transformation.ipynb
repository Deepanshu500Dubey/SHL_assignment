{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa1cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\deepd\\miniconda3\\lib\\site-packages (1.10.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de764dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\deepd\\miniconda3\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.3.51)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.3.23)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (2.0.37)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.2.11)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.10.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a173a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\deepd\\miniconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (2.10.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b49630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in c:\\users\\deepd\\miniconda3\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-groq) (0.3.51)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-groq) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.2.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from firecrawl import FirecrawlApp\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import pandas as pd\n",
    "import os  # Added for file existence check\n",
    "\n",
    "app = FirecrawlApp(api_key=\"fc-b325465df6d740c39fcb67ba0df51d82\")\n",
    "base_url = 'https://www.shl.com'\n",
    "csv_filename = 'shl_product_catalog120_243.csv'  # Define CSV filename once\n",
    "\n",
    "# Generate paginated URLs from start=12 to start=372 with step 12\n",
    "start_values = range(12, 373, 12)  # 12, 24, 36,..., 372\n",
    "urls = [f'https://www.shl.com/solutions/products/product-catalog/?start={start}&type=1&type=1' for start in start_values]\n",
    "\n",
    "headers = None  # Maintain headers state across pages\n",
    "\n",
    "for page_num, url in enumerate(urls, 1):\n",
    "    print(f\"\\n{'='*40}\\nProcessing Page {page_num} ({url})\\n{'='*40}\")\n",
    "    page_data = []  # Stores data for current page only\n",
    "    \n",
    "    try:\n",
    "        # Scrape catalog page\n",
    "        catalog_result = app.scrape_url(url, params={'formats': ['html']})\n",
    "        catalog_soup = BeautifulSoup(catalog_result['html'], 'html.parser')\n",
    "\n",
    "        # Process all tables on the page\n",
    "        for table in catalog_soup.find_all('table'):\n",
    "            # Extract headers only once\n",
    "            if headers is None:\n",
    "                headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "                headers += ['Description', 'Job Level', 'Language', 'Assessment Length']\n",
    "\n",
    "            # Process table rows\n",
    "            for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "                cols = row.find_all('td')\n",
    "                try:\n",
    "                    # Extract basic info\n",
    "                    solution_link = cols[0].find('a')\n",
    "                    solution_name = solution_link.get_text(strip=True) if solution_link else 'N/A'\n",
    "                    solution_url = urljoin(base_url, solution_link['href']) if solution_link else ''\n",
    "                    \n",
    "                    remote_testing = 'Yes' if cols[1].find('span') and '-yes' in cols[1].span.get('class', []) else 'No'\n",
    "                    adaptive_irt = 'Yes' if cols[2].find('span') and '-yes' in cols[2].span.get('class', []) else 'No'\n",
    "                    test_types = ' '.join([span.get_text(strip=True) for span in cols[3].find_all('span')])\n",
    "\n",
    "                    # Initialize default details\n",
    "                    details = {\n",
    "                        'Description': 'Not found',\n",
    "                        'Job Level': 'Not found',\n",
    "                        'Language': 'Not found',\n",
    "                        'Assessment Length': 'Not found'\n",
    "                    }\n",
    "\n",
    "                    # Scrape detailed product page\n",
    "                    if solution_url:\n",
    "                        try:\n",
    "                            product_result = app.scrape_url(solution_url, params={'formats': ['html']})\n",
    "                            product_soup = BeautifulSoup(product_result['html'], 'html.parser')\n",
    "                            \n",
    "                            for div in product_soup.find_all('div', class_='product-catalogue-training-calendar__row'):\n",
    "                                h4 = div.find('h4')\n",
    "                                p_tag = div.find('p')\n",
    "                                if h4 and p_tag:\n",
    "                                    key = h4.get_text(strip=True)\n",
    "                                    value = p_tag.get_text(strip=True).rstrip(', ')\n",
    "                                    \n",
    "                                    if 'description' in key.lower():\n",
    "                                        details['Description'] = value\n",
    "                                    elif 'job level' in key.lower():\n",
    "                                        details['Job Level'] = value\n",
    "                                    elif 'language' in key.lower():\n",
    "                                        details['Language'] = value\n",
    "                                    elif 'assessment length' in key.lower():\n",
    "                                        if minutes := re.search(r'\\d+', value):\n",
    "                                            details['Assessment Length'] = f\"{minutes.group()} minutes\"\n",
    "                            \n",
    "                            time.sleep(10)  # Reduced delay for scalability\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping {solution_url}: {str(e)}\")\n",
    "\n",
    "                    # Add row data to page_data\n",
    "                    page_data.append([\n",
    "                        f\"[{solution_name}]({solution_url})\" if solution_url else solution_name,\n",
    "                        remote_testing,\n",
    "                        adaptive_irt,\n",
    "                        test_types,\n",
    "                        details['Description'],\n",
    "                        details['Job Level'],\n",
    "                        details['Language'],\n",
    "                        details['Assessment Length']\n",
    "                    ])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        # Save page data immediately after processing\n",
    "        if headers and page_data:\n",
    "            df_page = pd.DataFrame(page_data, columns=headers)\n",
    "            \n",
    "            # Write to CSV with appropriate mode\n",
    "            if not os.path.isfile(csv_filename):\n",
    "                df_page.to_csv(csv_filename, index=False)\n",
    "                print(f\"Created new CSV with {len(df_page)} records from page {page_num}\")\n",
    "            else:\n",
    "                df_page.to_csv(csv_filename, mode='a', header=False, index=False)\n",
    "                print(f\"Appended {len(df_page)} records from page {page_num}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {url}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nScraping completed. Final data saved in:\", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad22d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# ======================\n",
    "# Precompute Embeddings\n",
    "# ======================\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def create_faiss_index(df: pd.DataFrame):\n",
    "    \"\"\"Generate and store embeddings in FAISS index\"\"\"\n",
    "    # Create embeddings\n",
    "    embeddings = model.encode(df['Skills_JobLevel'].tolist(), show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# ======================\n",
    "# Enhanced Recommendation\n",
    "# ======================\n",
    "def recommend_with_faiss(input_skills: str, df: pd.DataFrame, index):\n",
    "    \"\"\"FAISS-powered semantic search\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([input_skills]).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = index.search(query_embedding, k=50)\n",
    "    \n",
    "    # Get top matches from original DF\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['similarity'] = 1 - (distances[0] / 4)  # Convert L2 distance to similarity score\n",
    "    \n",
    "    return results.nlargest(10, 'similarity')\n",
    "\n",
    "import json\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# Set Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "class Supervisor:\n",
    "    def __init__(self, supervisor_name: str, supervisor_prompt: str, model: Any):\n",
    "        self.name = supervisor_name\n",
    "        self.prompt_template = supervisor_prompt\n",
    "        self.model = model\n",
    "\n",
    "    def format_prompt(self, team_members: List[str]) -> str:\n",
    "        return self.prompt_template.format(team_members=\", \".join(team_members))\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, worker_name: str, worker_prompt: str, supervisor: Supervisor, tools: Optional[List[Any]] = None):\n",
    "        self.name = worker_name\n",
    "        self.prompt_template = worker_prompt\n",
    "        self.supervisor = supervisor\n",
    "        self.tools = tools or []\n",
    "        \n",
    "    def clean_response(self, response: str) -> Any:  # Changed return type to Any\n",
    "        # Extract content after last </think> tag\n",
    "        if '</think>' in response:\n",
    "            response = response.split('</think>')[-1]\n",
    "        \n",
    "        # Remove markdown formatting and numbering\n",
    "        response = response.replace('**', '').strip()\n",
    "        response = response.split(':')[-1].strip()\n",
    "        \n",
    "        # Custom cleaning per worker type\n",
    "        if self.name == 'TestTypeAnalyst':\n",
    "            return ''.join([c for c in response if c.isupper() or c == ','])\n",
    "        elif self.name == 'Skill Extractor':\n",
    "            return '\\n'.join([s.split('. ')[-1] for s in response.split('\\n')])\n",
    "        elif self.name == 'Time Limit Identifier':\n",
    "            return response.split()[0]\n",
    "        elif self.name == 'Testing Type Identifier':\n",
    "            # Special handling for testing type response\n",
    "            response = response.strip('[]')\n",
    "            parts = [part.strip().lower() for part in response.split(',')]\n",
    "            return [part if part in ('yes', 'no') else 'no' for part in parts]\n",
    "        \n",
    "        return response.split('\\n')[0].strip('\"').strip()\n",
    "    \n",
    "    def process_input(self, user_input: str) -> str: # Added the missing process_input function\n",
    "        prompt = f\"{self.prompt_template}\\n\\nUser Input: {user_input}\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = self.supervisor.model.invoke(messages)\n",
    "        return self.clean_response(response.content)\n",
    "\n",
    "# Initialize Groq Chat Model\n",
    "groq_model = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Initialize Supervisor with 6 workers\n",
    "supervisor = Supervisor(\n",
    "    supervisor_name=\"AssessmentCoordinator\",\n",
    "    supervisor_prompt=\"You manage these specialists: {team_members}. Coordinate assessment creation workflow. Select next worker strategically. FINISH when complete.\",\n",
    "    model=groq_model\n",
    ")\n",
    "\n",
    "# Initialize 6 Specialist Workers\n",
    "workers = [\n",
    "    Worker(\n",
    "        worker_name=\"TestTypeAnalyst\",\n",
    "        worker_prompt='''You are an AI classifier that maps user inputs to test type codes from this taxonomy:\n",
    "\n",
    "Test Types (Code: Description)\n",
    "\n",
    "A: Ability & Aptitude (cognitive skills, problem-solving)\n",
    "\n",
    "B: Biodata & Situational Judgement (past behavior, hypothetical scenarios)\n",
    "\n",
    "C: Competencies (job-specific skills like leadership)\n",
    "\n",
    "D: Development & 360 (growth feedback, multi-rater reviews)\n",
    "\n",
    "E: Assessment Exercises (role-plays, case studies)\n",
    "\n",
    "K: Knowledge & Skills (technical/domain expertise)\n",
    "\n",
    "P: Personality & Behavior (traits, motivations)\n",
    "\n",
    "S: Simulations (realistic job-task replicas)\n",
    "\n",
    "Rules:\n",
    "\n",
    "Return only the relevant letter codes (e.g., K, A,S).\n",
    "\n",
    "Use commas for multiple matches (no spaces).\n",
    "\n",
    "Prioritize specificity (e.g., \"Python coding test\" â†’ K, not A).\n",
    "\n",
    "Default to B for biographical/historical scenarios.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"Quiz on Java and cloud architecture\" â†’ K\n",
    "\n",
    "Input: \"Test how someone leads a team during a crisis\" â†’ C,S\n",
    "\n",
    "Input: \"Evaluate agreeableness and reaction to feedback\" â†’ P,D\n",
    "\n",
    "Output Format:\n",
    "Return only the letter code(s) as a comma-separated string (e.g., P or B,S).\n",
    "\n",
    "''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Skill Extractor\",\n",
    "        worker_prompt='''You are a skill extractor for assessment design. Identify both hard and soft skills explicitly mentioned in the userâ€™s input that are relevant to the testâ€™s purpose.\n",
    "\n",
    "Rules:\n",
    "Focus: Extract hard skills (technical) and soft skills (non-technical):\n",
    "\n",
    "âœ… Hard Skills:\n",
    "\n",
    "Tools: Python, SQL, AWS\n",
    "\n",
    "Frameworks: TensorFlow, React\n",
    "\n",
    "Domains: cybersecurity, CAD, data analysis\n",
    "\n",
    "âœ… Soft Skills:\n",
    "\n",
    "communication, leadership, teamwork, problem-solving\n",
    "\n",
    "ðŸš« Exclude:\n",
    "\n",
    "Generic terms: \"experience,\" \"knowledge,\" \"proficiency\"\n",
    "\n",
    "Job roles: \"developer,\" \"engineer\"\n",
    "\n",
    "Test Type Context: Use the test type code (A/B/C/D/E/K/P/S) to refine extraction:\n",
    "\n",
    "Example: Test type K (Knowledge & Skills) â†’ Prioritize hard skills like Python.\n",
    "\n",
    "Example: Test type C (Competencies) â†’ Include both hard skills (CAD) and soft skills (leadership).\n",
    "\n",
    "Example: Test type P (Personality) â†’ Extract only soft skills if mentioned (e.g., adaptability).\n",
    "\n",
    "Normalization:\n",
    "\n",
    "Standardize terms: JS â†’ JavaScript, ML â†’ machine learning.\n",
    "\n",
    "Merge equivalents: CAD â†’ Computer-Aided Design.\n",
    "\n",
    "Output:\n",
    "\n",
    "Return a comma-separated list (e.g., Python, leadership, CAD).\n",
    "\n",
    "If no skills are found, return [].\n",
    "\n",
    "Examples:\n",
    "Input\tTest Type\tOutput\n",
    "â€œTest Python coding and teamwork.â€\tK\tPython, teamwork\n",
    "â€œAssess problem-solving and cloud architecture.â€\tA\tproblem-solving, cloud architecture\n",
    "â€œEvaluate leadership and CAD proficiency.â€\tC\tleadership, CAD\n",
    "â€œBehavioral test focusing on communication.â€\tP\tcommunication\n",
    "â€œNo skills mentioned.â€\tS\t[]''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Job Level Identifier\",\n",
    "        worker_prompt='''You are an AI assistant tasked with identifying the job level for which a test is intended. Given input that may include job titles, responsibilities, or descriptions, determine the most appropriate job level from the following list:\n",
    "\n",
    "Director\n",
    "\n",
    "Entry Level\n",
    "\n",
    "Executive\n",
    "\n",
    "Frontline Manager\n",
    "\n",
    "General Population\n",
    "\n",
    "Graduate\n",
    "\n",
    "Manager\n",
    "\n",
    "Mid-Professional\n",
    "\n",
    "Professional\n",
    "\n",
    "Professional Individual Contributor\n",
    "\n",
    "Supervisor\n",
    "\n",
    "Use contextual clues in the input to make an accurate classification. Respond only with the job level.\n",
    " ''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Language Preference Identifier\",\n",
    "        worker_prompt='''You are a language detector for assessments. Identify spoken (natural) languages (e.g., English, Mandarin, Spanish) explicitly mentioned in the userâ€™s input.\n",
    "\n",
    "Rules:\n",
    "\n",
    "Focus:\n",
    "\n",
    "Extract only natural languages (e.g., \"French\", \"Japanese\").\n",
    "\n",
    "Ignore programming languages (Python, Java), tools (SQL), or frameworks (React).\n",
    "\n",
    "Defaults:\n",
    "\n",
    "Return English if no spoken language is mentioned.\n",
    "\n",
    "For multi-language requests (e.g., \"English and Spanish\"), return a comma-separated list: English, Spanish.\n",
    "\n",
    "Output:\n",
    "\n",
    "Use full language names (e.g., \"German\" not \"Deutsch\").\n",
    "\n",
    "Case-insensitive (e.g., \"spanish\" â†’ Spanish).\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"Test must be in Portuguese.\" â†’ Output: Portuguese\n",
    "\n",
    "Input: \"Python coding test with instructions in Arabic.\" â†’ Output: Arabic\n",
    "\n",
    "Input: \"Math exam for Spanish-speaking students.\" â†’ Output: Spanish\n",
    "\n",
    "Input: \"Timed Java assessment.\" â†’ Output: English\n",
    "\n",
    "Respond only with the language name(s). No explanations.\n",
    "\n",
    "'''\n",
    ",\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Time Limit Identifier\",\n",
    "        worker_prompt='''You are an AI that extracts explicit test durations from user input.\n",
    "\n",
    "Rules\n",
    "Extract:\n",
    "\n",
    "Return exact phrases with a number + time unit (e.g., 90 minutes, 2.5 hrs, no more than 45 mins).\n",
    "\n",
    "Include comparative phrasing (e.g., under 1 hour, at least 20 minutes).\n",
    "\n",
    "Ignore:\n",
    "\n",
    "Deadlines (e.g., submit by Friday).\n",
    "\n",
    "Experience durations (e.g., 5 years of experience).\n",
    "\n",
    "Vague terms (e.g., timed test, time-sensitive).\n",
    "\n",
    "Output:\n",
    "\n",
    "For valid durations: Return them as a comma-separated list (e.g., 1 hour, 30 mins).\n",
    "\n",
    "For no valid durations: Return no time specified.\n",
    "\n",
    "Examples\n",
    "Input\tOutput\n",
    "\"Complete the test in 45 mins.\"\t45 mins\n",
    "\"Section A: 1 hour; Section B: 30 mins.\"\t1 hour, 30 mins\n",
    "\"Timed exam with no duration mentioned.\"\tno time specified\n",
    "\"Submit by 5 PM and allow up to 2 hrs.\"\t2 hrs\n",
    "\"Requires 3+ years of experience.\"\tno time specified\n",
    "Strict Constraints\n",
    "Never return explanations, formatting, or placeholders.\n",
    "\n",
    "Only return extracted durations or no time specified.\n",
    "\n",
    "''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Testing Type Identifier\",\n",
    "        worker_prompt='''You are an AI classifier that detects mentions of remote testing or adaptive testing/IRT in user inputs and returns a structured response.\n",
    "\n",
    "Rules\n",
    "Detection Logic:\n",
    "\n",
    "Remote Testing: yes if the exact phrase \"remote testing\" is present.\n",
    "\n",
    "Adaptive Testing: yes if \"adaptive testing\" or \"IRT\" (case-insensitive) is present.\n",
    "\n",
    "Default to no for missing terms.\n",
    "\n",
    "Output Format:\n",
    "\n",
    "Return [yes,yes] if both terms are present.\n",
    "\n",
    "Return [yes,no] if only remote testing is mentioned.\n",
    "\n",
    "Return [no,yes] if only adaptive testing/IRT is mentioned.\n",
    "\n",
    "Return [no,no] if neither is mentioned.\n",
    "\n",
    "Constraints:\n",
    "\n",
    "NO explanations, NO deviations from the format.\n",
    "\n",
    "Exact matches only (e.g., \"remote\" â‰  \"remote testing\").\n",
    "\n",
    "Examples\n",
    "Input\tOutput\n",
    "\"Conduct remote testing with IRT.\"\t[yes,yes]\n",
    "\"Use adaptive testing.\"\t[no,yes]\n",
    "\"Remote testing required.\"\t[yes,no]\n",
    "\"Timed onsite exam.\"\t[no,no]\n",
    "Command:\n",
    "Return ONLY the structured list ([yes,yes], [no,yes], etc.). No other text!''',\n",
    "        supervisor=supervisor\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv('/content/shl_product_catalog.csv')\n",
    "\n",
    "# ======================\n",
    "# Data Processing Functions\n",
    "# ======================\n",
    "\n",
    "def extract_skills(test_name: str, description: str, test_type: str) -> str:\n",
    "    \"\"\"Extract skills using Skill Extractor worker\n",
    "    Args:\n",
    "        test_name: Name of the individual test\n",
    "        description: Test description content\n",
    "        test_type: Type of test (e.g., Technical, Behavioral)\n",
    "    Returns:\n",
    "        Comma-separated string of extracted skills\n",
    "    \"\"\"\n",
    "    skill_extractor = next(w for w in workers if w.name == 'Skill Extractor')\n",
    "    input_text = f\"Individual Test Solutions: {test_name}\\nDescription: {description}\\nTest Type: {test_type}\"\n",
    "    return skill_extractor.process_input(input_text)\n",
    "\n",
    "def combine_skills_joblevel(skills: str, job_level: str) -> str:\n",
    "    \"\"\"Combine skills and job level into standardized format\n",
    "    Args:\n",
    "        skills: Raw skills string from extraction\n",
    "        job_level: Job level from original data\n",
    "    Returns:\n",
    "        Formatted combination string (skills + job level)\n",
    "    \"\"\"\n",
    "    cleaned_skills = skills.replace('[]', '').strip()\n",
    "    if cleaned_skills and job_level:\n",
    "        return f\"{cleaned_skills} , {job_level}\"\n",
    "    return cleaned_skills or job_level\n",
    "\n",
    "# ======================\n",
    "# Main Data Processing\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---- Skill Extraction ----\n",
    "    df['Skills'] = df.apply(\n",
    "        lambda row: extract_skills(\n",
    "            test_name=row['Individual Test Solutions'],\n",
    "            description=row['Description'],\n",
    "            test_type=row['Test Type']\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Skill-Job Level Combination ----\n",
    "    df['Skills_JobLevel'] = df.apply(\n",
    "        lambda row: combine_skills_joblevel(\n",
    "            skills=row['Skills'],\n",
    "            job_level=row['Job Level']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Data Cleaning ----\n",
    "    # Convert assessment length to numeric\n",
    "    df['Assessment Length'] = (\n",
    "        pd.to_numeric(\n",
    "            df['Assessment Length'].str.extract('(\\d+)', expand=False),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    # ---- Column Management ----\n",
    "    # Remove intermediate/original columns\n",
    "    df = df.drop(columns=['Description', 'Job Level', 'Skills'])\n",
    "    \n",
    "    # Define final column order\n",
    "    ordered_columns = [\n",
    "        'Individual Test Solutions',\n",
    "        'Remote Testing',\n",
    "        'Adaptive/IRT', \n",
    "        'Test Type',\n",
    "        'Skills_JobLevel',\n",
    "        'Language',\n",
    "        'Assessment Length'\n",
    "    ]\n",
    "    \n",
    "    # Reorganize dataframe\n",
    "    df = df[ordered_columns]\n",
    "    \n",
    "    # ======================\n",
    "    # Final Output\n",
    "    # ======================\n",
    "    print(\"\\nProcessed DataFrame Structure:\")\n",
    "    print(df.head(3).to_markdown(index=False))\n",
    "\n",
    "# ======================\n",
    "# Data Processing Functions\n",
    "# ======================\n",
    "\n",
    "def extract_skills(test_name: str, description: str, test_type: str) -> str:\n",
    "    \"\"\"Extract skills using Skill Extractor worker\"\"\"\n",
    "    skill_extractor = next(w for w in workers if w.name == 'Skill Extractor')\n",
    "    input_text = f\"Individual Test Solutions: {test_name}\\nDescription: {description}\\nTest Type: {test_type}\"\n",
    "    return skill_extractor.process_input(input_text)\n",
    "\n",
    "def combine_skills_joblevel(skills: str, job_level: str) -> str:\n",
    "    \"\"\"Combine skills and job level into standardized format\"\"\"\n",
    "    cleaned_skills = skills.replace('[]', '').strip()\n",
    "    if cleaned_skills and job_level:\n",
    "        return f\"{cleaned_skills} , {job_level}\"\n",
    "    return cleaned_skills or job_level\n",
    "\n",
    "# ======================\n",
    "# Main Data Processing\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---- Skill Extraction ----\n",
    "    df['Skills'] = df.apply(\n",
    "        lambda row: extract_skills(\n",
    "            test_name=row['Individual Test Solutions'],\n",
    "            description=row['Description'],\n",
    "            test_type=row['Test Type']\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Skill-Job Level Combination ----\n",
    "    df['Skills_JobLevel'] = df.apply(\n",
    "        lambda row: combine_skills_joblevel(\n",
    "            skills=row['Skills'],\n",
    "            job_level=row['Job Level']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Data Cleaning ----\n",
    "    df['Assessment Length'] = (\n",
    "        pd.to_numeric(\n",
    "            df['Assessment Length'].str.extract('(\\d+)', expand=False),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    # ---- Column Management ----\n",
    "    df = df.drop(columns=['Description', 'Job Level', 'Skills'])\n",
    "    ordered_columns = [\n",
    "        'Individual Test Solutions',\n",
    "        'Remote Testing',\n",
    "        'Adaptive/IRT', \n",
    "        'Test Type',\n",
    "        'Skills_JobLevel',\n",
    "        'Language',\n",
    "        'Assessment Length'\n",
    "    ]\n",
    "    df = df[ordered_columns]\n",
    "    \n",
    "    # ======================\n",
    "    # Save Processed Data\n",
    "    # ======================\n",
    "    output_filename = \"processed_assessments.csv\"\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nâœ… Successfully saved processed data to {output_filename}\")\n",
    "    print(f\"ðŸ“Š Total records saved: {len(df):,}\")\n",
    "    \n",
    "    # Optional: Show sample output\n",
    "    print(\"\\nSample of saved data:\")\n",
    "    print(df.head(3).to_markdown(index=False))\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ======================\n",
    "# Precompute Embeddings\n",
    "# ======================\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def create_faiss_index(df: pd.DataFrame):\n",
    "    \"\"\"Generate and store embeddings in FAISS index\"\"\"\n",
    "    # Create embeddings\n",
    "    embeddings = model.encode(df['Skills_JobLevel'].tolist(), show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# ======================\n",
    "# Enhanced Recommendation\n",
    "# ======================\n",
    "def recommend_with_faiss(input_skills: str, df: pd.DataFrame, index):\n",
    "    \"\"\"FAISS-powered semantic search\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([input_skills]).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = index.search(query_embedding, k=50)\n",
    "    \n",
    "    # Get top matches from original DF\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['similarity'] = 1 - (distances[0] / 4)  # Convert L2 distance to similarity score\n",
    "    \n",
    "    return results.nlargest(10, 'similarity')\n",
    "\n",
    "# preprocessing.py\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_and_save_index():\n",
    "    # Load processed data\n",
    "    df = pd.read_csv(\"/content/processed_assessments.csv\")  # Your cleaned dataset\n",
    "    \n",
    "    # Generate embeddings\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(df['Skills_JobLevel'].tolist(), show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Create optimized index\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    faiss.normalize_L2(embeddings)  # Crucial for cosine similarity\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Save index and metadata\n",
    "    faiss.write_index(index, \"precomputed_faiss_index.bin\")\n",
    "    df.to_parquet(\"metadata.parquet\")  # Faster read than CSV\n",
    "\n",
    "\n",
    "\n",
    "create_and_save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d1410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

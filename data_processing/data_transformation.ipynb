{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa1cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\deepd\\miniconda3\\lib\\site-packages (1.10.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de764dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\deepd\\miniconda3\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.3.51)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.3.23)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (2.0.37)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.2.11)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.10.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a173a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\deepd\\miniconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (2.10.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b49630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in c:\\users\\deepd\\miniconda3\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-groq) (0.3.51)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-groq) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.2.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepd\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from firecrawl import FirecrawlApp\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import pandas as pd\n",
    "import os  # Added for file existence check\n",
    "\n",
    "app = FirecrawlApp(api_key=\"fc-b325465df6d740c39fcb67ba0df51d82\")\n",
    "base_url = 'https://www.shl.com'\n",
    "csv_filename = 'shl_product_catalog120_243.csv'  # Define CSV filename once\n",
    "\n",
    "# Generate paginated URLs from start=12 to start=372 with step 12\n",
    "start_values = range(12, 373, 12)  # 12, 24, 36,..., 372\n",
    "urls = [f'https://www.shl.com/solutions/products/product-catalog/?start={start}&type=1&type=1' for start in start_values]\n",
    "\n",
    "headers = None  # Maintain headers state across pages\n",
    "\n",
    "for page_num, url in enumerate(urls, 1):\n",
    "    print(f\"\\n{'='*40}\\nProcessing Page {page_num} ({url})\\n{'='*40}\")\n",
    "    page_data = []  # Stores data for current page only\n",
    "    \n",
    "    try:\n",
    "        # Scrape catalog page\n",
    "        catalog_result = app.scrape_url(url, params={'formats': ['html']})\n",
    "        catalog_soup = BeautifulSoup(catalog_result['html'], 'html.parser')\n",
    "\n",
    "        # Process all tables on the page\n",
    "        for table in catalog_soup.find_all('table'):\n",
    "            # Extract headers only once\n",
    "            if headers is None:\n",
    "                headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "                headers += ['Description', 'Job Level', 'Language', 'Assessment Length']\n",
    "\n",
    "            # Process table rows\n",
    "            for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "                cols = row.find_all('td')\n",
    "                try:\n",
    "                    # Extract basic info\n",
    "                    solution_link = cols[0].find('a')\n",
    "                    solution_name = solution_link.get_text(strip=True) if solution_link else 'N/A'\n",
    "                    solution_url = urljoin(base_url, solution_link['href']) if solution_link else ''\n",
    "                    \n",
    "                    remote_testing = 'Yes' if cols[1].find('span') and '-yes' in cols[1].span.get('class', []) else 'No'\n",
    "                    adaptive_irt = 'Yes' if cols[2].find('span') and '-yes' in cols[2].span.get('class', []) else 'No'\n",
    "                    test_types = ' '.join([span.get_text(strip=True) for span in cols[3].find_all('span')])\n",
    "\n",
    "                    # Initialize default details\n",
    "                    details = {\n",
    "                        'Description': 'Not found',\n",
    "                        'Job Level': 'Not found',\n",
    "                        'Language': 'Not found',\n",
    "                        'Assessment Length': 'Not found'\n",
    "                    }\n",
    "\n",
    "                    # Scrape detailed product page\n",
    "                    if solution_url:\n",
    "                        try:\n",
    "                            product_result = app.scrape_url(solution_url, params={'formats': ['html']})\n",
    "                            product_soup = BeautifulSoup(product_result['html'], 'html.parser')\n",
    "                            \n",
    "                            for div in product_soup.find_all('div', class_='product-catalogue-training-calendar__row'):\n",
    "                                h4 = div.find('h4')\n",
    "                                p_tag = div.find('p')\n",
    "                                if h4 and p_tag:\n",
    "                                    key = h4.get_text(strip=True)\n",
    "                                    value = p_tag.get_text(strip=True).rstrip(', ')\n",
    "                                    \n",
    "                                    if 'description' in key.lower():\n",
    "                                        details['Description'] = value\n",
    "                                    elif 'job level' in key.lower():\n",
    "                                        details['Job Level'] = value\n",
    "                                    elif 'language' in key.lower():\n",
    "                                        details['Language'] = value\n",
    "                                    elif 'assessment length' in key.lower():\n",
    "                                        if minutes := re.search(r'\\d+', value):\n",
    "                                            details['Assessment Length'] = f\"{minutes.group()} minutes\"\n",
    "                            \n",
    "                            time.sleep(10)  # Reduced delay for scalability\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error scraping {solution_url}: {str(e)}\")\n",
    "\n",
    "                    # Add row data to page_data\n",
    "                    page_data.append([\n",
    "                        f\"[{solution_name}]({solution_url})\" if solution_url else solution_name,\n",
    "                        remote_testing,\n",
    "                        adaptive_irt,\n",
    "                        test_types,\n",
    "                        details['Description'],\n",
    "                        details['Job Level'],\n",
    "                        details['Language'],\n",
    "                        details['Assessment Length']\n",
    "                    ])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        # Save page data immediately after processing\n",
    "        if headers and page_data:\n",
    "            df_page = pd.DataFrame(page_data, columns=headers)\n",
    "            \n",
    "            # Write to CSV with appropriate mode\n",
    "            if not os.path.isfile(csv_filename):\n",
    "                df_page.to_csv(csv_filename, index=False)\n",
    "                print(f\"Created new CSV with {len(df_page)} records from page {page_num}\")\n",
    "            else:\n",
    "                df_page.to_csv(csv_filename, mode='a', header=False, index=False)\n",
    "                print(f\"Appended {len(df_page)} records from page {page_num}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {url}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nScraping completed. Final data saved in:\", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad22d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# ======================\n",
    "# Precompute Embeddings\n",
    "# ======================\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def create_faiss_index(df: pd.DataFrame):\n",
    "    \"\"\"Generate and store embeddings in FAISS index\"\"\"\n",
    "    # Create embeddings\n",
    "    embeddings = model.encode(df['Skills_JobLevel'].tolist(), show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# ======================\n",
    "# Enhanced Recommendation\n",
    "# ======================\n",
    "def recommend_with_faiss(input_skills: str, df: pd.DataFrame, index):\n",
    "    \"\"\"FAISS-powered semantic search\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([input_skills]).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = index.search(query_embedding, k=50)\n",
    "    \n",
    "    # Get top matches from original DF\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['similarity'] = 1 - (distances[0] / 4)  # Convert L2 distance to similarity score\n",
    "    \n",
    "    return results.nlargest(10, 'similarity')\n",
    "\n",
    "import json\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# Set Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "class Supervisor:\n",
    "    def __init__(self, supervisor_name: str, supervisor_prompt: str, model: Any):\n",
    "        self.name = supervisor_name\n",
    "        self.prompt_template = supervisor_prompt\n",
    "        self.model = model\n",
    "\n",
    "    def format_prompt(self, team_members: List[str]) -> str:\n",
    "        return self.prompt_template.format(team_members=\", \".join(team_members))\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, worker_name: str, worker_prompt: str, supervisor: Supervisor, tools: Optional[List[Any]] = None):\n",
    "        self.name = worker_name\n",
    "        self.prompt_template = worker_prompt\n",
    "        self.supervisor = supervisor\n",
    "        self.tools = tools or []\n",
    "        \n",
    "    def clean_response(self, response: str) -> Any:  # Changed return type to Any\n",
    "        # Extract content after last </think> tag\n",
    "        if '</think>' in response:\n",
    "            response = response.split('</think>')[-1]\n",
    "        \n",
    "        # Remove markdown formatting and numbering\n",
    "        response = response.replace('**', '').strip()\n",
    "        response = response.split(':')[-1].strip()\n",
    "        \n",
    "        # Custom cleaning per worker type\n",
    "        if self.name == 'TestTypeAnalyst':\n",
    "            return ''.join([c for c in response if c.isupper() or c == ','])\n",
    "        elif self.name == 'Skill Extractor':\n",
    "            return '\\n'.join([s.split('. ')[-1] for s in response.split('\\n')])\n",
    "        elif self.name == 'Time Limit Identifier':\n",
    "            return response.split()[0]\n",
    "        elif self.name == 'Testing Type Identifier':\n",
    "            # Special handling for testing type response\n",
    "            response = response.strip('[]')\n",
    "            parts = [part.strip().lower() for part in response.split(',')]\n",
    "            return [part if part in ('yes', 'no') else 'no' for part in parts]\n",
    "        \n",
    "        return response.split('\\n')[0].strip('\"').strip()\n",
    "    \n",
    "    def process_input(self, user_input: str) -> str: # Added the missing process_input function\n",
    "        prompt = f\"{self.prompt_template}\\n\\nUser Input: {user_input}\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = self.supervisor.model.invoke(messages)\n",
    "        return self.clean_response(response.content)\n",
    "\n",
    "# Initialize Groq Chat Model\n",
    "groq_model = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Initialize Supervisor with 6 workers\n",
    "supervisor = Supervisor(\n",
    "    supervisor_name=\"AssessmentCoordinator\",\n",
    "    supervisor_prompt=\"You manage these specialists: {team_members}. Coordinate assessment creation workflow. Select next worker strategically. FINISH when complete.\",\n",
    "    model=groq_model\n",
    ")\n",
    "\n",
    "# Initialize 6 Specialist Workers\n",
    "workers = [\n",
    "    Worker(\n",
    "        worker_name=\"TestTypeAnalyst\",\n",
    "        worker_prompt='''You are an AI classifier that maps user inputs to test type codes from this taxonomy:\n",
    "\n",
    "Test Types (Code: Description)\n",
    "\n",
    "A: Ability & Aptitude (cognitive skills, problem-solving)\n",
    "\n",
    "B: Biodata & Situational Judgement (past behavior, hypothetical scenarios)\n",
    "\n",
    "C: Competencies (job-specific skills like leadership)\n",
    "\n",
    "D: Development & 360 (growth feedback, multi-rater reviews)\n",
    "\n",
    "E: Assessment Exercises (role-plays, case studies)\n",
    "\n",
    "K: Knowledge & Skills (technical/domain expertise)\n",
    "\n",
    "P: Personality & Behavior (traits, motivations)\n",
    "\n",
    "S: Simulations (realistic job-task replicas)\n",
    "\n",
    "Rules:\n",
    "\n",
    "Return only the relevant letter codes (e.g., K, A,S).\n",
    "\n",
    "Use commas for multiple matches (no spaces).\n",
    "\n",
    "Prioritize specificity (e.g., \"Python coding test\" → K, not A).\n",
    "\n",
    "Default to B for biographical/historical scenarios.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"Quiz on Java and cloud architecture\" → K\n",
    "\n",
    "Input: \"Test how someone leads a team during a crisis\" → C,S\n",
    "\n",
    "Input: \"Evaluate agreeableness and reaction to feedback\" → P,D\n",
    "\n",
    "Output Format:\n",
    "Return only the letter code(s) as a comma-separated string (e.g., P or B,S).\n",
    "\n",
    "''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Skill Extractor\",\n",
    "        worker_prompt='''You are a skill extractor for assessment design. Identify both hard and soft skills explicitly mentioned in the user’s input that are relevant to the test’s purpose.\n",
    "\n",
    "Rules:\n",
    "Focus: Extract hard skills (technical) and soft skills (non-technical):\n",
    "\n",
    "✅ Hard Skills:\n",
    "\n",
    "Tools: Python, SQL, AWS\n",
    "\n",
    "Frameworks: TensorFlow, React\n",
    "\n",
    "Domains: cybersecurity, CAD, data analysis\n",
    "\n",
    "✅ Soft Skills:\n",
    "\n",
    "communication, leadership, teamwork, problem-solving\n",
    "\n",
    "🚫 Exclude:\n",
    "\n",
    "Generic terms: \"experience,\" \"knowledge,\" \"proficiency\"\n",
    "\n",
    "Job roles: \"developer,\" \"engineer\"\n",
    "\n",
    "Test Type Context: Use the test type code (A/B/C/D/E/K/P/S) to refine extraction:\n",
    "\n",
    "Example: Test type K (Knowledge & Skills) → Prioritize hard skills like Python.\n",
    "\n",
    "Example: Test type C (Competencies) → Include both hard skills (CAD) and soft skills (leadership).\n",
    "\n",
    "Example: Test type P (Personality) → Extract only soft skills if mentioned (e.g., adaptability).\n",
    "\n",
    "Normalization:\n",
    "\n",
    "Standardize terms: JS → JavaScript, ML → machine learning.\n",
    "\n",
    "Merge equivalents: CAD → Computer-Aided Design.\n",
    "\n",
    "Output:\n",
    "\n",
    "Return a comma-separated list (e.g., Python, leadership, CAD).\n",
    "\n",
    "If no skills are found, return [].\n",
    "\n",
    "Examples:\n",
    "Input\tTest Type\tOutput\n",
    "“Test Python coding and teamwork.”\tK\tPython, teamwork\n",
    "“Assess problem-solving and cloud architecture.”\tA\tproblem-solving, cloud architecture\n",
    "“Evaluate leadership and CAD proficiency.”\tC\tleadership, CAD\n",
    "“Behavioral test focusing on communication.”\tP\tcommunication\n",
    "“No skills mentioned.”\tS\t[]''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Job Level Identifier\",\n",
    "        worker_prompt='''You are an AI assistant tasked with identifying the job level for which a test is intended. Given input that may include job titles, responsibilities, or descriptions, determine the most appropriate job level from the following list:\n",
    "\n",
    "Director\n",
    "\n",
    "Entry Level\n",
    "\n",
    "Executive\n",
    "\n",
    "Frontline Manager\n",
    "\n",
    "General Population\n",
    "\n",
    "Graduate\n",
    "\n",
    "Manager\n",
    "\n",
    "Mid-Professional\n",
    "\n",
    "Professional\n",
    "\n",
    "Professional Individual Contributor\n",
    "\n",
    "Supervisor\n",
    "\n",
    "Use contextual clues in the input to make an accurate classification. Respond only with the job level.\n",
    " ''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Language Preference Identifier\",\n",
    "        worker_prompt='''You are a language detector for assessments. Identify spoken (natural) languages (e.g., English, Mandarin, Spanish) explicitly mentioned in the user’s input.\n",
    "\n",
    "Rules:\n",
    "\n",
    "Focus:\n",
    "\n",
    "Extract only natural languages (e.g., \"French\", \"Japanese\").\n",
    "\n",
    "Ignore programming languages (Python, Java), tools (SQL), or frameworks (React).\n",
    "\n",
    "Defaults:\n",
    "\n",
    "Return English if no spoken language is mentioned.\n",
    "\n",
    "For multi-language requests (e.g., \"English and Spanish\"), return a comma-separated list: English, Spanish.\n",
    "\n",
    "Output:\n",
    "\n",
    "Use full language names (e.g., \"German\" not \"Deutsch\").\n",
    "\n",
    "Case-insensitive (e.g., \"spanish\" → Spanish).\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"Test must be in Portuguese.\" → Output: Portuguese\n",
    "\n",
    "Input: \"Python coding test with instructions in Arabic.\" → Output: Arabic\n",
    "\n",
    "Input: \"Math exam for Spanish-speaking students.\" → Output: Spanish\n",
    "\n",
    "Input: \"Timed Java assessment.\" → Output: English\n",
    "\n",
    "Respond only with the language name(s). No explanations.\n",
    "\n",
    "'''\n",
    ",\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Time Limit Identifier\",\n",
    "        worker_prompt='''You are an AI that extracts explicit test durations from user input.\n",
    "\n",
    "Rules\n",
    "Extract:\n",
    "\n",
    "Return exact phrases with a number + time unit (e.g., 90 minutes, 2.5 hrs, no more than 45 mins).\n",
    "\n",
    "Include comparative phrasing (e.g., under 1 hour, at least 20 minutes).\n",
    "\n",
    "Ignore:\n",
    "\n",
    "Deadlines (e.g., submit by Friday).\n",
    "\n",
    "Experience durations (e.g., 5 years of experience).\n",
    "\n",
    "Vague terms (e.g., timed test, time-sensitive).\n",
    "\n",
    "Output:\n",
    "\n",
    "For valid durations: Return them as a comma-separated list (e.g., 1 hour, 30 mins).\n",
    "\n",
    "For no valid durations: Return no time specified.\n",
    "\n",
    "Examples\n",
    "Input\tOutput\n",
    "\"Complete the test in 45 mins.\"\t45 mins\n",
    "\"Section A: 1 hour; Section B: 30 mins.\"\t1 hour, 30 mins\n",
    "\"Timed exam with no duration mentioned.\"\tno time specified\n",
    "\"Submit by 5 PM and allow up to 2 hrs.\"\t2 hrs\n",
    "\"Requires 3+ years of experience.\"\tno time specified\n",
    "Strict Constraints\n",
    "Never return explanations, formatting, or placeholders.\n",
    "\n",
    "Only return extracted durations or no time specified.\n",
    "\n",
    "''',\n",
    "        supervisor=supervisor\n",
    "    ),\n",
    "    Worker(\n",
    "        worker_name=\"Testing Type Identifier\",\n",
    "        worker_prompt='''You are an AI classifier that detects mentions of remote testing or adaptive testing/IRT in user inputs and returns a structured response.\n",
    "\n",
    "Rules\n",
    "Detection Logic:\n",
    "\n",
    "Remote Testing: yes if the exact phrase \"remote testing\" is present.\n",
    "\n",
    "Adaptive Testing: yes if \"adaptive testing\" or \"IRT\" (case-insensitive) is present.\n",
    "\n",
    "Default to no for missing terms.\n",
    "\n",
    "Output Format:\n",
    "\n",
    "Return [yes,yes] if both terms are present.\n",
    "\n",
    "Return [yes,no] if only remote testing is mentioned.\n",
    "\n",
    "Return [no,yes] if only adaptive testing/IRT is mentioned.\n",
    "\n",
    "Return [no,no] if neither is mentioned.\n",
    "\n",
    "Constraints:\n",
    "\n",
    "NO explanations, NO deviations from the format.\n",
    "\n",
    "Exact matches only (e.g., \"remote\" ≠ \"remote testing\").\n",
    "\n",
    "Examples\n",
    "Input\tOutput\n",
    "\"Conduct remote testing with IRT.\"\t[yes,yes]\n",
    "\"Use adaptive testing.\"\t[no,yes]\n",
    "\"Remote testing required.\"\t[yes,no]\n",
    "\"Timed onsite exam.\"\t[no,no]\n",
    "Command:\n",
    "Return ONLY the structured list ([yes,yes], [no,yes], etc.). No other text!''',\n",
    "        supervisor=supervisor\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv('/content/shl_product_catalog.csv')\n",
    "\n",
    "# ======================\n",
    "# Data Processing Functions\n",
    "# ======================\n",
    "\n",
    "def extract_skills(test_name: str, description: str, test_type: str) -> str:\n",
    "    \"\"\"Extract skills using Skill Extractor worker\n",
    "    Args:\n",
    "        test_name: Name of the individual test\n",
    "        description: Test description content\n",
    "        test_type: Type of test (e.g., Technical, Behavioral)\n",
    "    Returns:\n",
    "        Comma-separated string of extracted skills\n",
    "    \"\"\"\n",
    "    skill_extractor = next(w for w in workers if w.name == 'Skill Extractor')\n",
    "    input_text = f\"Individual Test Solutions: {test_name}\\nDescription: {description}\\nTest Type: {test_type}\"\n",
    "    return skill_extractor.process_input(input_text)\n",
    "\n",
    "def combine_skills_joblevel(skills: str, job_level: str) -> str:\n",
    "    \"\"\"Combine skills and job level into standardized format\n",
    "    Args:\n",
    "        skills: Raw skills string from extraction\n",
    "        job_level: Job level from original data\n",
    "    Returns:\n",
    "        Formatted combination string (skills + job level)\n",
    "    \"\"\"\n",
    "    cleaned_skills = skills.replace('[]', '').strip()\n",
    "    if cleaned_skills and job_level:\n",
    "        return f\"{cleaned_skills} , {job_level}\"\n",
    "    return cleaned_skills or job_level\n",
    "\n",
    "# ======================\n",
    "# Main Data Processing\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---- Skill Extraction ----\n",
    "    df['Skills'] = df.apply(\n",
    "        lambda row: extract_skills(\n",
    "            test_name=row['Individual Test Solutions'],\n",
    "            description=row['Description'],\n",
    "            test_type=row['Test Type']\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Skill-Job Level Combination ----\n",
    "    df['Skills_JobLevel'] = df.apply(\n",
    "        lambda row: combine_skills_joblevel(\n",
    "            skills=row['Skills'],\n",
    "            job_level=row['Job Level']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Data Cleaning ----\n",
    "    # Convert assessment length to numeric\n",
    "    df['Assessment Length'] = (\n",
    "        pd.to_numeric(\n",
    "            df['Assessment Length'].str.extract('(\\d+)', expand=False),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    # ---- Column Management ----\n",
    "    # Remove intermediate/original columns\n",
    "    df = df.drop(columns=['Description', 'Job Level', 'Skills'])\n",
    "    \n",
    "    # Define final column order\n",
    "    ordered_columns = [\n",
    "        'Individual Test Solutions',\n",
    "        'Remote Testing',\n",
    "        'Adaptive/IRT', \n",
    "        'Test Type',\n",
    "        'Skills_JobLevel',\n",
    "        'Language',\n",
    "        'Assessment Length'\n",
    "    ]\n",
    "    \n",
    "    # Reorganize dataframe\n",
    "    df = df[ordered_columns]\n",
    "    \n",
    "    # ======================\n",
    "    # Final Output\n",
    "    # ======================\n",
    "    print(\"\\nProcessed DataFrame Structure:\")\n",
    "    print(df.head(3).to_markdown(index=False))\n",
    "\n",
    "# ======================\n",
    "# Data Processing Functions\n",
    "# ======================\n",
    "\n",
    "def extract_skills(test_name: str, description: str, test_type: str) -> str:\n",
    "    \"\"\"Extract skills using Skill Extractor worker\"\"\"\n",
    "    skill_extractor = next(w for w in workers if w.name == 'Skill Extractor')\n",
    "    input_text = f\"Individual Test Solutions: {test_name}\\nDescription: {description}\\nTest Type: {test_type}\"\n",
    "    return skill_extractor.process_input(input_text)\n",
    "\n",
    "def combine_skills_joblevel(skills: str, job_level: str) -> str:\n",
    "    \"\"\"Combine skills and job level into standardized format\"\"\"\n",
    "    cleaned_skills = skills.replace('[]', '').strip()\n",
    "    if cleaned_skills and job_level:\n",
    "        return f\"{cleaned_skills} , {job_level}\"\n",
    "    return cleaned_skills or job_level\n",
    "\n",
    "# ======================\n",
    "# Main Data Processing\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---- Skill Extraction ----\n",
    "    df['Skills'] = df.apply(\n",
    "        lambda row: extract_skills(\n",
    "            test_name=row['Individual Test Solutions'],\n",
    "            description=row['Description'],\n",
    "            test_type=row['Test Type']\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Skill-Job Level Combination ----\n",
    "    df['Skills_JobLevel'] = df.apply(\n",
    "        lambda row: combine_skills_joblevel(\n",
    "            skills=row['Skills'],\n",
    "            job_level=row['Job Level']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # ---- Data Cleaning ----\n",
    "    df['Assessment Length'] = (\n",
    "        pd.to_numeric(\n",
    "            df['Assessment Length'].str.extract('(\\d+)', expand=False),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    # ---- Column Management ----\n",
    "    df = df.drop(columns=['Description', 'Job Level', 'Skills'])\n",
    "    ordered_columns = [\n",
    "        'Individual Test Solutions',\n",
    "        'Remote Testing',\n",
    "        'Adaptive/IRT', \n",
    "        'Test Type',\n",
    "        'Skills_JobLevel',\n",
    "        'Language',\n",
    "        'Assessment Length'\n",
    "    ]\n",
    "    df = df[ordered_columns]\n",
    "    \n",
    "    # ======================\n",
    "    # Save Processed Data\n",
    "    # ======================\n",
    "    output_filename = \"processed_assessments.csv\"\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\n✅ Successfully saved processed data to {output_filename}\")\n",
    "    print(f\"📊 Total records saved: {len(df):,}\")\n",
    "    \n",
    "    # Optional: Show sample output\n",
    "    print(\"\\nSample of saved data:\")\n",
    "    print(df.head(3).to_markdown(index=False))\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ======================\n",
    "# Precompute Embeddings\n",
    "# ======================\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def create_faiss_index(df: pd.DataFrame):\n",
    "    \"\"\"Generate and store embeddings in FAISS index\"\"\"\n",
    "    # Create embeddings\n",
    "    embeddings = model.encode(df['Skills_JobLevel'].tolist(), show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# ======================\n",
    "# Enhanced Recommendation\n",
    "# ======================\n",
    "def recommend_with_faiss(input_skills: str, df: pd.DataFrame, index):\n",
    "    \"\"\"FAISS-powered semantic search\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([input_skills]).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = index.search(query_embedding, k=50)\n",
    "    \n",
    "    # Get top matches from original DF\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['similarity'] = 1 - (distances[0] / 4)  # Convert L2 distance to similarity score\n",
    "    \n",
    "    return results.nlargest(10, 'similarity')\n",
    "\n",
    "# preprocessing.py\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_and_save_index():\n",
    "    # Load processed data\n",
    "    df = pd.read_csv(\"/content/processed_assessments.csv\")  # Your cleaned dataset\n",
    "    \n",
    "    # Generate embeddings\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(df['Skills_JobLevel'].tolist(), show_progress_bar=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Create optimized index\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    faiss.normalize_L2(embeddings)  # Crucial for cosine similarity\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Save index and metadata\n",
    "    faiss.write_index(index, \"precomputed_faiss_index.bin\")\n",
    "    df.to_parquet(\"metadata.parquet\")  # Faster read than CSV\n",
    "\n",
    "\n",
    "\n",
    "create_and_save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d1410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
